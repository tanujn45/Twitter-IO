{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5aoIMTYZA84Oc7dz/Yn7c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanujn45/Twitter-IO/blob/main/Twitter_IO_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## !pip3 install wget\n",
        "## !pip3 install httpx\n",
        "## !pip3 install StringIO\n",
        "!pip3 uninstall pillow"
      ],
      "metadata": {
        "id": "Ww7GyzxugR9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "538ca43a-0e3c-47d0-fc96-25a8a541a158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: Pillow 9.2.0\n",
            "Uninstalling Pillow-9.2.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/PIL/*\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow-9.2.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow.libs/libXau-00ec42fe.so.6.0.0\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow.libs/libfreetype-27ef11b8.so.6.18.3\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow.libs/libharfbuzz-aa5f3c5c.so.0.40401.0\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow.libs/libjpeg-3e290cba.so.62.3.0\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow.libs/liblcms2-1e643a89.so.2.0.13\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow.libs/liblzma-d540a118.so.5.2.5\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow.libs/libopenjp2-fca9bf24.so.2.5.0\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow.libs/libpng16-52f22300.so.16.37.0\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow.libs/libtiff-3a0dc242.so.5.8.0\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow.libs/libwebp-8efe125f.so.7.1.3\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow.libs/libwebpdemux-016472e8.so.2.0.9\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow.libs/libwebpmux-5c00cf3e.so.3.0.8\n",
            "    /usr/local/lib/python3.7/dist-packages/Pillow.libs/libxcb-1122e22b.so.1.1.0\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Pillow-9.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pillow"
      ],
      "metadata": {
        "id": "e0vU4jNaYVft",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8f0a8cf9-d105-46d4-9ac1-3de30f767e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pillow\n",
            "  Using cached Pillow-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Installing collected packages: pillow\n",
            "Successfully installed pillow-9.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr tesseract-ocr-all\n",
        "!pip3 install pytesseract"
      ],
      "metadata": {
        "id": "qZSqsqI7UhAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMu_JUZIfHrO"
      },
      "outputs": [],
      "source": [
        "## import wget\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "from os.path import normpath, exists, basename\n",
        "from io import BytesIO\n",
        "from urllib.parse import urlparse\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import pytesseract\n",
        "import re\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!sudo apt-get update -qq 2>&1 > /dev/null\n",
        "!sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "!google-drive-ocamlfuse\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwZueOuF_X8I",
        "outputId": "ebe1a3d2-18ad-47cd-a264-ab0b12cc30d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: www-browser: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links2: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: elinks: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: lynx: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: w3m: not found\n",
            "xdg-open: no method available for opening 'https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=ilpI0WEHIGxr1J0fS7yGVqFsGV9vMPjS6Mg0rFOG%2FD0'\n",
            "/bin/sh: 1: firefox: not found\n",
            "/bin/sh: 1: google-chrome: not found\n",
            "/bin/sh: 1: chromium-browser: not found\n",
            "/bin/sh: 1: open: not found\n",
            "Cannot retrieve auth tokens.\n",
            "Failure(\"Error opening URL:https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=ilpI0WEHIGxr1J0fS7yGVqFsGV9vMPjS6Mg0rFOG%2FD0\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -qq w3m # to act as web browser\n",
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y671mvHCVtR",
        "outputId": "40936624-176e-4cc7-e2c1-27efe63cc754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libgpm2:amd64.\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 156363 files and directories currently installed.)\n",
            "Preparing to unpack .../libgpm2_1.20.7-5_amd64.deb ...\n",
            "Unpacking libgpm2:amd64 (1.20.7-5) ...\n",
            "Selecting previously unselected package w3m.\n",
            "Preparing to unpack .../w3m_0.5.3-36build1_amd64.deb ...\n",
            "Unpacking w3m (0.5.3-36build1) ...\n",
            "Setting up libgpm2:amd64 (1.20.7-5) ...\n",
            "Setting up w3m (0.5.3-36build1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "/content\n",
            "/content/drive\n",
            "/content\n",
            "/\n",
            "Access token retrieved correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " lang = {'Afrikaans': 'afr', 'Amharic': 'amh', 'Arabic': 'ara', 'Assamese': 'asm', 'Azerbaijani': 'aze', 'Azerbaijani - Cyrillic': 'aze_cyrl', 'Belarusian': 'bel', 'Bengali': 'ben', 'Tibetan': 'bod', 'Bosnian': 'bos', 'Bulgarian': 'bul', 'Catalan; Valencian': 'cat', 'Cebuano': 'ceb', 'Czech': 'ces', 'Chinese - Simplified': 'chi_sim', 'Chinese - Traditional': 'chi_tra', 'Cherokee': 'chr', 'Welsh': 'cym', 'Danish': 'dan', 'German': 'deu', 'Dzongkha': 'dzo', 'Greek, Modern (1453-)': 'ell', 'English': 'eng', 'English, Middle (1100-1500)': 'enm', 'Esperanto': 'epo', 'Estonian': 'est', 'Basque': 'eus', 'Persian': 'fas', 'Finnish': 'fin', 'French': 'fra', 'German Fraktur': 'frk', 'French, Middle (ca. 1400-1600)': 'frm', 'Irish': 'gle', 'Galician': 'glg', 'Greek, Ancient (-1453)': 'grc', 'Gujarati': 'guj', 'Haitian; Haitian Creole': 'hat', 'Hebrew': 'heb', 'Hindi': 'hin', 'Croatian': 'hrv', 'Hungarian': 'hun', 'Inuktitut': 'iku', 'Indonesian': 'ind', 'Icelandic': 'isl', 'Italian': 'ita', 'Italian - Old': 'ita_old', 'Javanese': 'jav', 'Japanese': 'jpn', 'Kannada': 'kan', 'Georgian': 'kat', 'Georgian - Old': 'kat_old', 'Kazakh': 'kaz', 'Central Khmer': 'khm', 'Kirghiz; Kyrgyz': 'kir', 'Korean': 'kor', 'Kurdish': 'kur', 'Lao': 'lao', 'Latin': 'lat', 'Latvian': 'lav', 'Lithuanian': 'lit', 'Malayalam': 'mal', 'Marathi': 'mar', 'Macedonian': 'mkd', 'Maltese': 'mlt', 'Malay': 'msa', 'Burmese': 'mya', 'Nepali': 'nep', 'Dutch; Flemish': 'nld', 'Norwegian': 'nor', 'Oriya': 'ori', 'Panjabi; Punjabi': 'pan', 'Polish': 'pol', 'Portuguese': 'por', 'Pushto; Pashto': 'pus', 'Romanian; Moldavian; Moldovan': 'ron', 'Russian': 'rus', 'Sanskrit': 'san', 'Sinhala; Sinhalese': 'sin', 'Slovak': 'slk', 'Slovenian': 'slv', 'Spanish; Castilian': 'spa', 'Spanish; Castilian - Old': 'spa_old', 'Albanian': 'sqi', 'Serbian': 'srp', 'Serbian - Latin': 'srp_latn', 'Swahili': 'swa', 'Swedish': 'swe', 'Syriac': 'syr', 'Tamil': 'tam', 'Telugu': 'tel', 'Tajik': 'tgk', 'Tagalog': 'tgl', 'Katakana': 'kan', 'Cyrillic': 'aze_cyrl', 'Devanagari' : 'hin', 'Han' : 'kor', 'Fraktur': 'frm' }"
      ],
      "metadata": {
        "id": "jZvK1O_keNUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to fetch name of the file from the URL\n",
        "def URLFetchName(URL):\n",
        "  a = urlparse(URL)\n",
        "  return basename(a.path)"
      ],
      "metadata": {
        "id": "wmpg7-2C0kWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LocationBasename(loc):\n",
        "  return basename(normpath(loc))"
      ],
      "metadata": {
        "id": "-X1mZ0bUZkEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_files(path):\n",
        "  for file in os.listdir(path):\n",
        "    if os.path.isfile(os.path.join(path, file)):\n",
        "      yield file"
      ],
      "metadata": {
        "id": "o_nFSBn4yXMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to retrieve SerialIds from media txt file\n",
        "def MediaSerialId(loc):\n",
        "  f = open(loc, 'r')\n",
        "  serialIds = f.readlines()\n",
        "  SerialIds = []\n",
        "  SerialId = []\n",
        "  for i in range(len(serialIds) - 1, 0, -1):\n",
        "    if(serialIds[i] == '\\n'):\n",
        "      continue\n",
        "    if '******' in serialIds[i]:\n",
        "      SerialId.reverse()\n",
        "      SerialIds.append(SerialId)\n",
        "      SerialId = []\n",
        "      continue\n",
        "    SerialId.append(serialIds[i][:-1])\n",
        "  SerialIds.reverse()\n",
        "  return SerialIds"
      ],
      "metadata": {
        "id": "MZ2z2zb2gFVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fix bad zip files\n",
        "def FixBadZipfile(zipFile):\n",
        "  f = open(zipFile, 'r+b')\n",
        "  data = f.read()\n",
        "  pos = data.find(b'\\x50\\x4b\\x05\\x06')\n",
        "  if(pos > 0):\n",
        "    print(\"Truncating file at location \" + str(pos + 22) + \".\")\n",
        "    f.seek(pos + 22)\n",
        "    f.truncate()\n",
        "    f.close()\n",
        "  else:\n",
        "    print('Something went wrong with fixing the Zip file!')"
      ],
      "metadata": {
        "id": "j1BI72mRtyAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fetch tweetid and user id dictionary from a csv file\n",
        "def FetchTweetUserId(fileName):\n",
        "  df = pd.read_csv(fileName)\n",
        "  data = pd.Series(df.userid.values, index=df.tweetid).to_dict()\n",
        "  return data"
      ],
      "metadata": {
        "id": "LkDFQnwlJkKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "client = storage.Client.create_anonymous_client()\n",
        "\n",
        "bucket_name = 'twitter-election-integrity'\n",
        "\n",
        "bucket = client.bucket(bucket_name)\n",
        "# blob = bucket.blob('hashed/2021_12/Venezuela_0621/Venezuela_0621_tweets_csv_hashed.zip')\n",
        "# blob.download_to_filename('/content/tweets/zipfile.zip')"
      ],
      "metadata": {
        "id": "eG_g5x3EMexS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_image_lang(img_path):\n",
        "    try:\n",
        "        osd = pytesseract.image_to_osd(img_path)\n",
        "        script = re.search(\"Script: ([a-zA-Z]+)\\n\", osd).group(1)\n",
        "        conf = re.search(\"Script confidence: (\\d+\\.?(\\d+)?)\", osd).group(1)\n",
        "        return script, float(conf)\n",
        "    except:\n",
        "        return None, 0.0\n",
        "\n",
        "# script_name, confidence = detect_image_lang('/content/tweets/unZip/egypt_032020_hashed_016/839445738/1051936668989739009-Dpk1iGyVAAA5_eS.jpg')\n",
        "# print(script_name, confidence)"
      ],
      "metadata": {
        "id": "jmHKggEoUl7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob = bucket.blob('hashed/2020_08/cuba_082020/cuba_082020_tweets_csv_hashed.zip')\n",
        "blob.download_to_filename('/content/cuba_082020_tweets_csv_hashed.zip')\n",
        "with ZipFile('/content/cuba_082020_tweets_csv_hashed.zip', 'r') as zip:\n",
        "  print('Extracting all the files now...')\n",
        "  zip.extractall('/content/')\n",
        "  print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YiVZ0hYiya1",
        "outputId": "d0bb63b2-d46b-4e60-d626-88b69cd91885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting all the files now...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob = bucket.blob('hashed/2021_12/Venezuela_0621/Venezuela_0621_tweets_csv_hashed.zip')\n",
        "blob.download_to_filename('/content/Venezuela_0621_tweets_csv_hashed.zip')\n",
        "with ZipFile('/content/Venezuela_0621_tweets_csv_hashed.zip', 'r') as zip:\n",
        "  print('Extracting all the files now...')\n",
        "  zip.extractall('/content/')\n",
        "  print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MwIgk_7jCOF",
        "outputId": "442fe45b-8bbc-4d04-f0a2-9474f222d276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting all the files now...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob = bucket.blob('hashed/2020_08/cuba_082020/cuba_082020_tweet_media_hashed/cuba_082020_hashed_README.txt')\n",
        "blob.download_to_filename('/content/cuba_082020_hashed_README.txt')"
      ],
      "metadata": {
        "id": "e93EliUDB0bC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob = bucket.blob('hashed/2021_12/Venezuela_0621/Venezuela_0621_tweet_media_hashed/Venezuela_0621_hashed_README.txt')\n",
        "blob.download_to_filename('/content/Venezuela_0621_hashed_README.txt')"
      ],
      "metadata": {
        "id": "pBnd8SzSi1VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shutil.rmtree('/content/drive/MyDrive/Twitter IO/cuba_venezuela_2/')\n",
        "\n",
        "os.mkdir('/content/drive/MyDrive/Twitter IO/cuba_venezuela_3')\n",
        "os.mkdir('/content/drive/MyDrive/Twitter IO/cuba_venezuela_3/results')\n",
        "os.mkdir('/content/drive/MyDrive/Twitter IO/cuba_venezuela_3/images')\n",
        "os.mkdir('/content/drive/MyDrive/Twitter IO/cuba_venezuela_3/videos')\n"
      ],
      "metadata": {
        "id": "xJ40ktZhVIyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_to_check_csv = '/content/cuba_venezuela_3.csv'\n",
        "tweet_csv = '/content/cuba_082020_tweets_csv_hashed.csv'\n",
        "media_txt = '/content/cuba_082020_hashed_README.txt'\n",
        "SerialIds = MediaSerialId(media_txt)\n",
        "\n",
        "f = open('/content/cuba_venezuela3.csv', 'w', encoding='UTF8')\n",
        "not_found = open('/content/cuba_venezuela_not_found3.csv', 'w', encoding='UTF8')\n",
        "writer = csv.writer(f)\n",
        "writer.writerow(['zip_num', 'userId', 'tweetId'])\n",
        "not_found_writer = csv.writer(not_found)\n",
        "\n",
        "for chunk_parent in pd.read_csv(tweet_to_check_csv, chunksize=1000):\n",
        "    for index_parent, row_parent in chunk_parent.iterrows():\n",
        "        userId = row_parent['actor_1']\n",
        "        tweetId = row_parent['tweet_id']\n",
        "        data = []\n",
        "        try:\n",
        "            [(i, _)] = [(i, serialId.index(userId)) for i, serialId in enumerate(SerialIds) if userId in serialId]\n",
        "            data = [i + 1, userId, tweetId]\n",
        "            writer.writerow(data)\n",
        "        except:\n",
        "            not_found_writer.writerow([userId, tweetId])\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "qfkRa_TzZ06P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "with open('/content/cuba_venezuela1.csv') as f:\n",
        "    data = sorted(csv.reader(f))\n",
        "# write to the output file\n",
        "with open('/content/cuba_venezuela_output1.csv', 'w', newline='\\n') as f:\n",
        "    csv.writer(f).writerows(data)"
      ],
      "metadata": {
        "id": "3K7eTXyWzdEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "just_text = open('/content/drive/MyDrive/Twitter IO/cuba_venezuela_2/just_text', 'w')\n",
        "unknown_lang = open('/content/drive/MyDrive/Twitter IO/cuba_venezuela_2/unknown_lang', 'w')\n",
        "csv_sorted_with_zip = '/content/cuba_venezuela2.csv'\n",
        "curr_location = -1000\n",
        "\n",
        "for chunk_parent in pd.read_csv(csv_sorted_with_zip, chunksize=1000):\n",
        "    for index_parent, row_parent in chunk_parent.iterrows():\n",
        "        location = row_parent['zip_num']\n",
        "        userId = row_parent['userId']\n",
        "        tweetId = str(row_parent['tweetId'])\n",
        "\n",
        "        if location != curr_location:\n",
        "            # Downloading Zip\n",
        "            print('Downloading zip!')\n",
        "            blob = bucket.blob('hashed/2020_08/cuba_082020/cuba_082020_tweet_media_hashed/cuba_082020_hashed_' + str(location).rjust(3, '0') + '.zip')\n",
        "            blob.download_to_filename('/content/tweets/tweetZip.zip')\n",
        "            print('Download Complete!')\n",
        "\n",
        "            # Unzipping\n",
        "            with ZipFile('/content/tweets/tweetZip.zip', 'r') as zip:\n",
        "                print('Extracting all the files now...')\n",
        "                zip.extractall(path='/content/tweets/unZip')\n",
        "                print('Done!')\n",
        "\n",
        "            curr_location = location\n",
        "\n",
        "        print('UserId: ', userId, 'TweetId: ', tweetId)\n",
        "        unzipLocation = '/content/tweets/unZip/cuba_082020_hashed_' + str(location).rjust(3, '0') + '/' + userId\n",
        "        for file_name in get_files(unzipLocation):\n",
        "            if tweetId in file_name:\n",
        "                print(file_name + ' found!')\n",
        "                # Creating a text file for recognized text\n",
        "                # Adding TweetId, UserId\n",
        "                txt = open('/content/drive/MyDrive/Twitter IO/cuba_venezuela_2/results/' + tweetId + '.txt', 'w')\n",
        "                txt.write('####TWEET-ID#### ' + tweetId + '\\n')\n",
        "                txt.write('####USER-ID#### ' + userId + '\\n\\n')\n",
        "\n",
        "                # Going through all the images\n",
        "                # image_dir = '/content/tweets/unZip/cuba_082020_hashed_' + str(location).rjust(3, '0') + '/' + userId\n",
        "                # for filename in os.scandir(image_dir):\n",
        "                #   if filename.is_file():\n",
        "                image_path = unzipLocation + '/' + file_name\n",
        "\n",
        "                # Check if the file is an image\n",
        "                if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    print(LocationBasename(image_path))\n",
        "                    #Add image name, language and text detected\n",
        "                    txt.write('####IMAGE#### ' + LocationBasename(image_path) + '\\n')\n",
        "                    language, _ = detect_image_lang(image_path)\n",
        "                    if(language == None or language not in lang):\n",
        "                        lang_code = 'eng'\n",
        "                    else:\n",
        "                        lang_code = lang[language]\n",
        "                    if(language not in lang and language != None):\n",
        "                        unknown_lang.write(language + '\\n' + image_path + '\\n\\n\\n')\n",
        "\n",
        "                    txt.write('####LANG#### ' + lang_code + '\\n')\n",
        "                    txt.write('####TXT-START####\\n')\n",
        "                    shutil.copy(image_path, '/content/drive/MyDrive/Twitter IO/cuba_venezuela_2/images/')\n",
        "                    cont = ''\n",
        "                    try:\n",
        "                        cont = pytesseract.image_to_string(Image.open(image_path), lang=lang_code)\n",
        "                    except UnidentifiedImageError:\n",
        "                        print('Image Error')\n",
        "                    txt.write(cont)\n",
        "                    just_text.write(cont)\n",
        "                    just_text.write('\\n###################\\n')\n",
        "                    txt.write('\\n####TXT-END####\\n\\n')\n",
        "                    # Close the text file and remove the unzip directory and zip file to save space\n",
        "                    txt.close()\n",
        "                else:\n",
        "                    shutil.copy(image_path, '/content/drive/MyDrive/Twitter IO/cuba_venezuela_2/videos/')\n",
        "                break\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "c3LEwk1Bzz0M",
        "outputId": "293451e8-aff1-44d6-b25b-59048ea55e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EmptyDataError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-18e0eb21553f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcurr_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk_parent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_sorted_with_zip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex_parent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_parent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_parent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zip_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_to_check_csv = '/content/cuba_iran.csv'\n",
        "tweet_csv = '/content/cuba_082020_tweets_csv_hashed.csv'\n",
        "media_txt = '/content/cuba_082020_hashed_README.txt'\n",
        "SerialIds = MediaSerialId(media_txt)\n",
        "\n",
        "unknown_lang = open('/content/unknown_lang', 'w')\n",
        "done_tweetId = open('/content/done_tweetId', 'w')\n",
        "just_text = open('/content/just_text', 'w')\n",
        "\n",
        "# Iterating through all the tweetIds\n",
        "for chunk_parent in pd.read_csv(tweet_to_check_csv, chunksize=1000):\n",
        "  for index_parent, row_parent in chunk_parent.iterrows():\n",
        "\n",
        "    tweetId = str(row_parent['tweet_id'])\n",
        "    done_tweetId.write(tweetId + '\\n')\n",
        "    found = False\n",
        "    if(exists('/content/results/' + tweetId + '.txt')):\n",
        "      continue\n",
        "    print('TweetId:', tweetId)\n",
        "\n",
        "    for chunk in pd.read_csv(tweet_csv, chunksize=1000):\n",
        "      for index, row in chunk.iterrows():\n",
        "\n",
        "        # Searching for Userid using the TweetId\n",
        "        if(str(row['tweetid']) == tweetId):\n",
        "          print('User Id: ', row['userid'], 'TweetId: ', tweetId)\n",
        "          userId = row['userid']\n",
        "\n",
        "          # Finding the location of zip file using the SerialId\n",
        "          [(i, _)] = [(i, serialId.index(userId)) for i, serialId in enumerate(SerialIds) if userId in serialId]\n",
        "          location = i + 1\n",
        "          print('Zip file number: ', location)\n",
        "          print('Downloading Zip')\n",
        "          # Downloading the zip file\n",
        "          blob = bucket.blob('hashed/2020_08/cuba_082020/cuba_082020_tweet_media_hashed/cuba_082020_hashed_' + str(location).rjust(3, '0') + '.zip')\n",
        "          blob.download_to_filename('/content/tweets/tweetZip.zip')\n",
        "          print('Download Complete')\n",
        "          # Unzipping\n",
        "          with ZipFile('/content/tweets/tweetZip.zip', 'r') as zip:\n",
        "            print('Extracting all the files now...')\n",
        "            zip.extractall(path='/content/tweets/unZip')\n",
        "            print('Done!')\n",
        "\n",
        "\n",
        "          unzipLocation = '/content/tweets/unZip/cuba_082020_hashed_' + str(location).rjust(3, '0') + '/' + userId\n",
        "          # Checking for the tweetid in images\n",
        "          for file_name in get_files(unzipLocation):\n",
        "            print(file_name)\n",
        "            if tweetId in file_name:\n",
        "              # Creating a text file for recognized text\n",
        "              # Adding TweetId, UserId\n",
        "              txt = open('/content/results/' + tweetId + '.txt', 'w')\n",
        "              txt.write('####TWEET-ID#### ' + tweetId + '\\n')\n",
        "              txt.write('####USER-ID#### ' + userId + '\\n\\n')\n",
        "\n",
        "              # Going through all the images\n",
        "              # image_dir = '/content/tweets/unZip/cuba_082020_hashed_' + str(location).rjust(3, '0') + '/' + userId\n",
        "              # for filename in os.scandir(image_dir):\n",
        "              #   if filename.is_file():\n",
        "              image_path = unzipLocation + file_name\n",
        "\n",
        "              # Check if the file is an image\n",
        "              if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "\n",
        "                print(LocationBasename(image_path))\n",
        "                #Add image name, language and text detected\n",
        "                txt.write('####IMAGE#### ' + LocationBasename(image_path) + '\\n')\n",
        "                language, _ = detect_image_lang(image_path)\n",
        "                if(language == None or language not in lang):\n",
        "                  lang_code = 'eng'\n",
        "                else:\n",
        "                  lang_code = lang[language]\n",
        "\n",
        "                if(language not in lang and language != None):\n",
        "                  unknown_lang.write(language + '\\n' + image_path + '\\n\\n\\n')\n",
        "\n",
        "                txt.write('####LANG#### ' + lang_code + '\\n')\n",
        "                txt.write('####TXT-START####\\n')\n",
        "                shutil.copy(image_path, '/content/drive/MyDrive/Twitter IO/Cuba1')\n",
        "                cont = ''\n",
        "                try:\n",
        "                  cont = pytesseract.image_to_string(Image.open(image_path), lang=lang_code)\n",
        "                except UnidentifiedImageError:\n",
        "                  print('Image Error')\n",
        "                txt.write(cont)\n",
        "                just_text.write(cont)\n",
        "                just_text.write('\\n###################\\n')\n",
        "                txt.write('\\n####TXT-END####\\n\\n')\n",
        "              break\n",
        "\n",
        "          # Close the text file and remove the unzip directory and zip file to save space\n",
        "          txt.close()\n",
        "          shutil.rmtree('/content/tweets/unZip')\n",
        "          os.remove('/content/tweets/tweetZip.zip')\n",
        "          found = True\n",
        "          break\n",
        "        else:\n",
        "          # Do something if the UserId is not found\n",
        "          pass\n",
        "      if(found):\n",
        "        break\n",
        "\n",
        "done_tweetId.close()\n",
        "unknown_lang.close()\n",
        "just_text.close()"
      ],
      "metadata": {
        "id": "nod6nzLVpBP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile('/content/hashed_2020_04_egypt_022020_egypt_022020_tweets_csv_hashed.zip', 'r') as zip:\n",
        "  print('Extracting all the files now...')\n",
        "  zip.extractall(path='/content/tweets/unZip')\n",
        "  print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAVc8627Eiss",
        "outputId": "30847833-35c3-4160-bc76-272dc0f281d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting all the files now...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = '/content/tweets/egypt_032020_hashed_' + str(location).rjust(3, '0') + '/' + userId\n",
        "for filename in os.scandir(image_dir):\n",
        "  if filename.is_file():\n",
        "    image_path = filename.path\n",
        "    if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "      print(image_path)"
      ],
      "metadata": {
        "id": "TqWsDCIfIERD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.rmtree('/content/tweets/unZip/egypt_032020_hashed_014')"
      ],
      "metadata": {
        "id": "Yi_aMnCEGVaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.remove('/content/tweets/tweetZip.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "5huRVUk6Ruy9",
        "outputId": "39814195-62bb-4167-8bdc-057e6ab42537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-c8422d4ffaa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/tweets/tweetZip.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/tweets/tweetZip.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "language, confi = detect_image_lang('/content/tweets/unZip/egypt_032020_hashed_017/869891243999887360/870031588196855812-DBL5HZ4WAAEfoB6.jpg')\n",
        "print(language, confi)\n",
        "lang_code = lang[language]\n",
        "print(pytesseract.image_to_string(Image.open('/content/tweets/unZip/egypt_032020_hashed_017/869891243999887360/870031588196855812-DBL5HZ4WAAEfoB6.jpg'), lang='ara'))\n",
        "print(pytesseract.image_to_pdf_or_hocr('/content/tweets/unZip/egypt_032020_hashed_017/869891243999887360/870031588196855812-DBL5HZ4WAAEfoB6.jpg', lang='ara', extension='hocr'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uo0EYxpGWwD",
        "outputId": "d6308498-fea5-42b1-9f3e-06a2c25d65c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arabic 1.67\n",
            " \n",
            "\f\n",
            "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\\n    \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\\n <head>\\n  <title></title>\\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\" />\\n  <meta name=\\'ocr-system\\' content=\\'tesseract 4.0.0-beta.1\\' />\\n  <meta name=\\'ocr-capabilities\\' content=\\'ocr_page ocr_carea ocr_par ocr_line ocrx_word\\'/>\\n</head>\\n<body>\\n  <div class=\\'ocr_page\\' id=\\'page_1\\' title=\\'image \"/content/tweets/unZip/egypt_032020_hashed_017/869891243999887360/870031588196855812-DBL5HZ4WAAEfoB6.jpg\"; bbox 0 0 720 428; ppageno 0\\'>\\n   <div class=\\'ocr_carea\\' id=\\'block_1_1\\' title=\"bbox 0 0 0 428\">\\n    <p class=\\'ocr_par\\' id=\\'par_1_1\\' lang=\\'ara\\' title=\"bbox 0 0 0 428\">\\n     <span class=\\'ocr_line\\' id=\\'line_1_1\\' title=\"bbox 0 0 0 428; x_size 536.25; x_descenders 134.0625; x_ascenders 134.0625\"><span class=\\'ocrx_word\\' id=\\'word_1_1\\' title=\\'bbox 0 0 0 428; x_wconf 95\\'> </span> \\n     </span>\\n    </p>\\n   </div>\\n  </div>\\n </body>\\n</html>\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.rmtree('/content/tweets')\n",
        "os.mkdir('/content/tweets')"
      ],
      "metadata": {
        "id": "IKe0uUuxatv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_to_check_csv = '/content/egypt_egypt_uae.csv'\n",
        "\n",
        "for chunk_parent in pd.read_csv(tweet_to_check_csv, chunksize=1000):\n",
        "  for index_parent, row_parent in chunk_parent.iterrows():\n",
        "\n",
        "    if(row_parent['tweet_id'] == 1034523965534949378):\n",
        "      print(row_parent['tweet_id'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xidez8SDkn3z",
        "outputId": "7d5cb7a4-bf75-4c28-ae8c-f40dedfb6c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1034523965534949378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fileloc = '/content/zipfile.zip'\n",
        "zipfile = ZipFile(fileloc)\n",
        "zipfile.namelist()"
      ],
      "metadata": {
        "id": "Tk2QxiKVGg_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##!gsutil ls gs://twitter-election-integrity/hashed/2021_12/CNHU_0621/CNHU_0621_tweet_media_hashed/\n",
        "##!gsutil cp gs://twitter-election-integrity/hashed/2021_12/CNHU_0621/CNHU_0621_tweet_media_hashed/CNHU_0621_hashed_001.zip /content/"
      ],
      "metadata": {
        "id": "vZJiOUv_FQTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blobs_specific = list(bucket.list_blobs(prefix='hashed', delimiter='/'))\n",
        "blobs_specific"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l7KDGtZ566F",
        "outputId": "f544c2bd-83aa-42e5-d51a-dfe332ff4551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yearsHashed = list_directories(bucket_name, 'hashed/')\n",
        "years = []\n",
        "for year in yearsHashed:\n",
        "  years.append(LocationBasename(year))\n",
        "\n",
        "df = pd.read_csv('/content/all.csv')\n",
        "total = 0\n",
        "temp = []\n",
        "df = df.sort_values('Country_x')\n",
        "for index, row in df.iterrows():\n",
        "  #Checking for the year of dataset release\n",
        "  for year in years:\n",
        "    if year == row['year']:\n",
        "      blobLoc = '/hashed/' + year + '/'\n",
        "      #Fetching all the dataset names for that year\n",
        "      datasets = list_directories(bucket_name, blobLocc )\n",
        "      if row['dataset'] not in datasets:\n",
        "        print('invalid dataset')\n",
        "        break\n",
        "      for dataset in datasets:\n",
        "        if dataset == row['dataset']:\n",
        "          tweetZipFileName = dataset + '_tweets_csv_hashed.zip'\n",
        "          tweetCSVblob = blobLoc + dataset + '/' + tweetZipFileName\n",
        "          #Dataset exits. Download the tweetInfo file if it does not already exists\n",
        "          if not exists('/content/tweets/' + tweetZipFileName):\n",
        "            blob = bucket.blob(tweetCSVblob)\n",
        "            blob.download_to_filename(tweetZipFileName)\n",
        "          UserId = ''\n",
        "          #Iterate through the zip files to find the UserId associated with the TweetId\n",
        "          with ZipFile('/content/tweets/' + tweetZipFileName, \"r\") as f:\n",
        "            for name in f.namelist():\n",
        "                df = pd.read_csv(f.open(name))\n",
        "                data = pd.Series(df.userid.values, index=df.tweetid).to_dict()\n",
        "                if row['tweetId'] in data:\n",
        "                  UserId = data[row['tweetId']]\n",
        "                  break\n",
        "          #Downloading the readme file for that dataset\n",
        "          mediaBlobLoc = dataset + '_tweet_media_hashed/'\n",
        "          mediaReadmeBlob = bucket.blob(mediaBlobLoc + dataset + '_hashed_README.txt')\n",
        "          mediaReadmeLoc = '/content/readmes/' + dataset + '_hashed_README.txt'\n",
        "          blob.download_to_filename(mediaReadmeLoc)\n",
        "          #Finding the media zip file where the SerialId exists\n",
        "          serialIds = MediaSerialId(mediaReadmeLoc)\n",
        "          (i, _) = [(i, serialId.index(UserId)) for i, serialId in enumerate(serialIds) if UserId in serialId]\n",
        "          mediaZipBlobLoc = mediaBlobLoc + dataset +  '_hashed_' + str(i).rjust(3, '0') + '.zip'\n",
        "          mediaZibBlob = bucket.blob(mediaZipBlobLoc)\n",
        "          mediaZipLoc = '/content/mediaZip/' + + dataset +  '_hashed_' + str(i).rjust(3, '0') + '.zip'\n",
        "          mediaZibBlob.download_to_filename()\n",
        "          with ZipFile(mediaZipLoc, 'r') as zip:\n",
        "            print('Extracting all the files now...')\n",
        "            zip.extractall('/content/mediaZip/')\n",
        "            print('Done!')\n",
        "          #Zip file extracted. Now move into the desired folder and run the algorithm\n",
        "          serialIdLoc = mediaZipLoc[:-4] + '/' + UserId + '/'\n",
        "          #iterate through all the files. If the file is an image recognise text\n",
        "      break\n",
        "    else:\n",
        "      print('Invalid year for dataset!')\n",
        ""
      ],
      "metadata": {
        "id": "4OGzRgpux-rS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}